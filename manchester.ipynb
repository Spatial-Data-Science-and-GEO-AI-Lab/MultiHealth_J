{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system packages\n",
    "from itertools import islice\n",
    "import pickle\n",
    "import gc \n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# non-geo numeric packages\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing.pool import ThreadPool \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "import pandas as pd\n",
    "\n",
    "# network and OSM packages\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "city_geo = ox.geocoder.geocode_to_gdf\n",
    "\n",
    "# Earth engine packages\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# General geo-packages\n",
    "from rasterstats import zonal_stats\n",
    "from pyproj import CRS\n",
    "import libpysal\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from shapely.geometry import Point, MultiLineString, LineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest gdf\n",
    "popgridmanchester = gpd.read_file('popgridmanchester3.gpkg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEE auth/initialisation, loading health outcome data, extracting population grid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and Initialize Google Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided dataset including health outcomes\n",
    "data = gpd.read_file(\"/Users/Julian/Desktop/Julian/PSY/Thesis/data/Greater_Manchester_NH_GIS.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health outcome only\n",
    "data_health = data[['geometry','lsoa11cd','UNID','lsoa11nm','ID','lon','lat','Yrpotlife','Comilldis','Obes_Per_Obesprev_y15', 'Asthma_Per_Asthmaprev_y17', 'heart_Per_heartdiseaseprev_y17', 'Stroke_Per_Strokeprev_y17', 'Cancer_Per_cancerPrev_y17', 'samhi_index2019', 'prop_ibesa', 'est_qof_dep', 'antidep_rate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cities using the excel files with the cities and iso's\n",
    "\n",
    "# Extract iso-3166 country codes\n",
    "iso = pd.read_excel('/Users/Julian/Desktop/Julian/PSY/Thesis/data/iso_countries.xlsx')\n",
    "\n",
    "# Extract cities list\n",
    "cities = pd.read_excel('/Users/Julian/Desktop/Julian/PSY/Thesis/data/cities.xlsx') # all cities\n",
    "\n",
    "# 'cities_adj' serves by default as city-input for functions\n",
    "cities_adj = cities[cities['City'].isin(['Manchester'])]\n",
    "cities_adj = cities_adj.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract worldpop data from GEE of the cities\n",
    "def gee_worldpop_extract (city_file, iso, save_path = None):\n",
    "    \n",
    "    cities = city_file\n",
    "    \n",
    "    # Get included city areas\n",
    "    OSM_incl = [cities[cities['City'] == city]['OSM_area'].tolist()[0].rsplit(', ') for city in cities['City'].tolist()]\n",
    "\n",
    "    # Get the city geoms\n",
    "    obj = [city_geo(city).dissolve()['geometry'].tolist()[0] for city in OSM_incl]\n",
    "    \n",
    "    # Get the city countries\n",
    "    obj_displ = [city_geo(city).dissolve()['display_name'].tolist()[0].rsplit(', ')[-1]for city in OSM_incl]\n",
    "    obj_displ = np.where(pd.Series(obj_displ).str.contains(\"Ivoire\"),\"CIte dIvoire\",obj_displ)\n",
    "\n",
    "    # Get the country's iso-code\n",
    "    iso_list = [iso[iso['name'] == ob]['alpha3'].tolist()[0] for ob in obj_displ]\n",
    "\n",
    "    # Based on the iso-code return the worldpop 2020\n",
    "    ee_worldpop = [ee.ImageCollection(\"WorldPop/GP/100m/pop\")\\\n",
    "        .filter(ee.Filter.date('2020'))\\\n",
    "        .filter(ee.Filter.inList('country', [io])).first() for io in iso_list]\n",
    "\n",
    "    # Clip the countries with the city geoms.\n",
    "    clipped = [ee_worldpop[i].clip(shapely.geometry.mapping(obj[i])) for i in range(0,len(obj))]\n",
    "\n",
    "    # Create path if non-existent\n",
    "    if save_path == None:\n",
    "        path = ''\n",
    "    else:\n",
    "        path = save_path\n",
    "        if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "\n",
    "    # Export as TIFF file.\n",
    "    # Stored in form path + USA_Los Angeles_2020.tif\n",
    "    filenames = [path+iso_list[i]+'_'+cities['City'][i]+'_2020.tif' for i in range(len(obj))]\n",
    "    [geemap.ee_export_image(clipped[i], filename = filenames[i]) for i in range(0,len(obj))]\n",
    "    return(filenames)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now grid based on the 27700 CRS\n",
    "\n",
    "# function to create population grids of the cities\n",
    "def city_grids_format(city_grids, cities_area, grid_size = 100):\n",
    "    start_time = time.time()\n",
    "    grids = []\n",
    "    print(str(grid_size) + 'm resolution grids extraction')\n",
    "    for i in range(len(city_grids)):\n",
    "        \n",
    "        # Open the raster file\n",
    "        with rasterio.open(city_grids[i]) as src:\n",
    "            band= src.read() # the population values\n",
    "            aff = src.transform # the raster bounds and size (affine)\n",
    "        \n",
    "        # Get the rowwise arrays, get a 2D dataframe\n",
    "        grid = pd.DataFrame()\n",
    "        for b in enumerate(band[0]):\n",
    "            grid = pd.concat([grid, pd.Series(b[1],name=b[0])],axis=1)\n",
    "        grid= grid.unstack().reset_index()\n",
    "        \n",
    "        # Unstack df to columns\n",
    "        grid.columns = ['row','col','value']\n",
    "        grid['minx'] = aff[2]+aff[0]*grid['col']\n",
    "        grid['miny'] = aff[5]+aff[4]*grid['row']\n",
    "        grid['maxx'] = aff[2]+aff[0]*grid['col']+aff[0]\n",
    "        grid['maxy'] = aff[5]+aff[4]*grid['row']+aff[4]\n",
    "        \n",
    "        # Create polygon from affine bounds and row/col indices\n",
    "        grid['geometry'] = [Polygon([(grid.minx[i],grid.miny[i]),\n",
    "                                   (grid.maxx[i],grid.miny[i]),\n",
    "                                   (grid.maxx[i],grid.maxy[i]),\n",
    "                                   (grid.minx[i],grid.maxy[i])])\\\n",
    "                          for i in range(len(grid))]\n",
    "        \n",
    "        # Set the df as geo-df\n",
    "        grid = gpd.GeoDataFrame(grid, crs = 27700) \n",
    "\n",
    "        # Get dissolvement_key for dissolvement. \n",
    "        grid['row3'] = np.floor(grid['row']/(grid_size/100)).astype(int)\n",
    "        grid['col3'] = np.floor(grid['col']/(grid_size/100)).astype(int)\n",
    "        grid['dissolve_key'] = grid['row3'].astype(str) +'-'+ grid['col3'].astype(str)\n",
    "        \n",
    "        # Define a city's OSM area as Polygon.\n",
    "        geo_ls = gpd.GeoSeries(city_geo(cities_area[i].split(', ')).dissolve().geometry)\n",
    "        geo_ls = geo_ls.to_crs(\"EPSG:27700\")\n",
    "        \n",
    "        # Intersect grids with the city boundary Polygon.\n",
    "        insec = grid.intersection(geo_ls.tolist()[0])\n",
    "        \n",
    "        # Exclude grids outside the specified city boundaries\n",
    "        insec = insec[insec.area > 0]\n",
    "        \n",
    "        # Join in other information.\n",
    "        insec = gpd.GeoDataFrame(geometry = insec, crs = 27700).join(grid.loc[:, grid.columns != 'geometry'])\n",
    "        \n",
    "        # Dissolve into block by block grids\n",
    "        popgrid = insec[['dissolve_key','geometry','row3','col3']].dissolve('dissolve_key')\n",
    "        \n",
    "        # Get those grids populations and area, only full blocks\n",
    "        popgrid['population'] = round(insec.groupby('dissolve_key')['value'].sum()).astype(int)\n",
    "        #popgrid['area_m'] = round(gpd.GeoSeries(popgrid['geometry'], crs = 27700).to_crs(3043).area).astype(int)\n",
    "        popgrid = popgrid[popgrid['population'] >= 0]\n",
    "        #popgrid = popgrid[popgrid['area_m'] / popgrid['area_m'].max() > 0.95]\n",
    "\n",
    "        # Get centroids and coords\n",
    "        popgrid['centroid'] = popgrid['geometry'].centroid\n",
    "        #popgrid['centroid_m'] = gpd.GeoSeries(popgrid['centroid'], crs = '27700').to_crs(3043)\n",
    "        popgrid['grid_lon'] = popgrid['centroid'].x\n",
    "        popgrid['grid_lat'] = popgrid['centroid'].y\n",
    "        popgrid = popgrid.reset_index()\n",
    "\n",
    "        minx = popgrid.bounds['minx']\n",
    "        maxx = popgrid.bounds['maxx']\n",
    "        miny = popgrid.bounds['miny']\n",
    "        maxy = popgrid.bounds['maxy']\n",
    "\n",
    "        # Some geometries result in a multipolygon when dissolving (like i.e. 0.05 meters), coords error.\n",
    "        # Therefore recreate the polygon.\n",
    "        Poly = []\n",
    "        for k in range(len(popgrid)):\n",
    "            Poly.append(Polygon([(minx[k],maxy[k]),(maxx[k],maxy[k]),(maxx[k],miny[k]),(minx[k],miny[k])]))\n",
    "        popgrid['geometry'] = Poly\n",
    "\n",
    "        grids.append(popgrid)\n",
    "\n",
    "        print(city_grids[i].rsplit('_')[3], round((time.time() - start_time)/60,2),'mns')\n",
    "    return(grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required preprocess for information extraction\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# In essence, we use Google Earth Engine to extract a country's grid raster and clip it with the city's preferred OSM area\n",
    "# Predifine in Excel: the (1) city name as \"City\" and (2) the OSM area that needs to be extracted as \"OSM_area\"\n",
    "# i.e. City = \"Los Angeles\" and OSM_area = \"Los Angeles county, Orange county CA\"\n",
    "files = gee_worldpop_extract(cities_adj, iso, '/Users/Julian/Desktop/Julian/PSY/Thesis/data/GEE_city_grids/')\n",
    "\n",
    "# Files are downloaded automatically to the specified path. Files are also stored in Google with a downloadlink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprojecting the population TIF with the code snippit from the rasterio documentation\n",
    "\n",
    "# destination CRS\n",
    "dst_crs = 'EPSG:27700'\n",
    "\n",
    "# open the source GeoTIFF file and transform the dimensions and CRS\n",
    "with rasterio.open('/Users/Julian/Desktop/Julian/PSY/Thesis/data/GEE_city_grids/GBR_Manchester_2020.tif') as src:\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "    # save the new destination GeoTIFF file with the adjusted dimensions and CRS\n",
    "    with rasterio.open('/Users/Julian/Desktop/Julian/PSY/Thesis/data/GEE_city_grids/GBR_Manchester_2020.tif', 'w', **kwargs) as dst:\n",
    "        for i in range(1, src.count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, i),\n",
    "                destination=rasterio.band(dst, i),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the population grid\n",
    "\n",
    "# clip cities from countries, format population grids\n",
    "population_grids = city_grids_format(files,\n",
    "                                     cities_adj['OSM_area'],\n",
    "                                     grid_size = 100) # aggregating upwards to i.e. 200m, 300m etc. is possible\n",
    "print(' ')\n",
    "\n",
    "# save the dataframe\n",
    "popgridmanchester = population_grids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding grid_lat/lon from 4326 for OSMNX network retrieval\n",
    "popgridmanchester = popgridmanchester.to_crs(\"EPSG:4326\")\n",
    "popgridmanchester['centroid'] = popgridmanchester.geometry.centroid\n",
    "popgridmanchester['grid_lon_4326'] = popgridmanchester['centroid'].x\n",
    "popgridmanchester['grid_lat_4326'] = popgridmanchester['centroid'].y\n",
    "popgridmanchester = popgridmanchester.to_crs(\"EPSG:27700\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove centroid to be able to save it as a GPKG\n",
    "popgridmanchester = popgridmanchester.drop(columns=['centroid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as gkpg file\n",
    "popgridmanchester.to_file(\"popgridmanchester3.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open gkpg file\n",
    "#popgridmanchester = gpd.read_file(\"popgridmanchester.gpkg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data points such as bus stops and restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Greater Manchester polygon using OSM\n",
    "manchester_osm = city_geo('Greater Manchester')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using OSM to extract data points\n",
    "\n",
    "# specifying correct ESPG\n",
    "epsg = \"EPSG:27700\"\n",
    "\n",
    "# specifying the Greater Manchester polygon\n",
    "manchester_poly = manchester_osm.geometry.unary_union\n",
    "\n",
    "# extracting bus stops\n",
    "bus_stops = ox.geometries_from_polygon(manchester_poly, tags={'highway': 'bus_stop'})\n",
    "bus_stops = bus_stops.to_crs(epsg)\n",
    "bus_stops = bus_stops[['geometry']]\n",
    "bus_stops = bus_stops.reset_index()\n",
    "\n",
    "# extracting restaurants\n",
    "restaurants = ox.geometries_from_polygon(manchester_poly, tags={'amenity': ['bar', 'pub', 'restaurant', 'cafe']})\n",
    "restaurants = restaurants.to_crs(epsg)\n",
    "restaurants = restaurants[['geometry']]\n",
    "restaurants = restaurants.reset_index()\n",
    "\n",
    "# extracting daily shops\n",
    "shops = ox.geometries_from_polygon(manchester_poly, tags={'shop': ['department_store', 'supermarket', 'convenience']})\n",
    "shops.to_crs(epsg)\n",
    "shops = shops[['geometry']]\n",
    "shops = shops.reset_index()\n",
    "\n",
    "# extracting cyclelanes\n",
    "cycleways = ox.geometries_from_polygon(manchester_poly, tags={'cycleway': True, 'highway':'cycleway'})\n",
    "cycleways.to_crs(epsg)\n",
    "cycleways_c = cycleways[['geometry']]\n",
    "cycleways_c = cycleways_c.reset_index()\n",
    "\n",
    "#Create union out of all geometry extracted\n",
    "temp_list = []\n",
    "\n",
    "for index, x in cycleways_c.iterrows():\n",
    "    temp_list.append(x.geometry)\n",
    "\n",
    "series = gpd.GeoSeries(temp_list)\n",
    "\n",
    "# convert to gdf and export as gpkg\n",
    "cycleways_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(series))\n",
    "#cycleways_gdf.to_file(\"cycleways.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population sum of each grid bufferzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bufferzone dataframe from population grid dataframe geometries\n",
    "buffer_gdf = popgridmanchester.copy()\n",
    "buffer_gdf['centroid'] = buffer_gdf['geometry'].centroid\n",
    "buffer_gdf['buffer'] = buffer_gdf['centroid'].buffer(500)\n",
    "buffer_gdf = buffer_gdf['buffer']\n",
    "buffer_gdf = gpd.GeoDataFrame(buffer_gdf, geometry='buffer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect with bounds of the OSM city area so buffers don't extent beyond city borders\n",
    "manpoly = city_geo('Greater Manchester').dissolve()['geometry']\n",
    "manpoly = manpoly.to_crs(\"EPSG:27700\")\n",
    "\n",
    "def get_intersection(gdf, other):\n",
    "    return gdf.intersection(other)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p1 = ThreadPool(2)\n",
    "    bufferintersect = p1.starmap(get_intersection, [(buffer_gdf[0:1], manpoly.geometry[0])])\n",
    "    p1.terminate()\n",
    "    p1.join()\n",
    "    p2 = ThreadPool(4)\n",
    "    start_time = time.time()\n",
    "    bufferintersect = p2.starmap(get_intersection, [(buffer_gdf, manpoly.geometry[0])])\n",
    "    p2.terminate()\n",
    "    p2.join()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "# I tried intersecting with a dissolved polygon from the population grid intstead of with a dissolved\n",
    "# polygon from the OSM city area, because the population grid polygon contains small areas with no data\n",
    "# that would then not be present within the buffers. Using rasterstats with that intersection would then \n",
    "# minimize the amount of missing data set to a negative number. This took too much time however.\n",
    "# Might try again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to gdf and save to gpkg\n",
    "bufferintersect_gdf = gpd.GeoDataFrame(bufferintersect)\n",
    "bufferintersect_gdf = bufferintersect_gdf.T\n",
    "bufferintersect_gdf = bufferintersect_gdf.rename(columns={0: \"geometry\"})\n",
    "bufferintersect_gdf.geometry = bufferintersect_gdf['geometry']\n",
    "bufferintersect_gdf = gpd.GeoDataFrame(bufferintersect_gdf)\n",
    "bufferintersect_gdf.to_file(\"bufferintersect_gdf.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the buffergintersect_gdf file\n",
    "bufferintersect_gdf = gpd.read_file(\"bufferintersect_gdf.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parts from the buffer that overlap with the population grid parts with no data\n",
    "start_time = time.time()\n",
    "\n",
    "bufferintersect_gdf['savedindex'] = bufferintersect_gdf.index\n",
    "difference_gdf = bufferintersect_gdf.overlay(popgridmanchester, how='difference')['savedindex']\n",
    "\n",
    "# filter the buffer gdf of these difference parts\n",
    "buffer_gdf_filtered = bufferintersect_gdf[~bufferintersect_gdf.savedindex.isin(difference_gdf.geometry)]\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# use the reprojected population TIF with CRS 27700\n",
    "with rasterio.open('/Users/Julian/Desktop/Julian/PSY/Thesis/data/GEE_city_grids/GBR_Manchester_2020.tif') as src:\n",
    "    affine = src.transform\n",
    "    array = src.read(1)\n",
    "\n",
    "# get population sum statistic per bufferzone\n",
    "statistics = zonal_stats(bufferintersect_gdf, array, affine=affine, stats=['sum'], nodata=0)\n",
    "    \n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the population sum per buffer from statistics\n",
    "# setting values to zero if negative\n",
    "population_sums = [stat['sum'] if (stat is not None and stat['sum'] >= 0) else 0 for stat in statistics]\n",
    "# converting to df to add to the population grid gdf\n",
    "population_sums_df = pd.DataFrame(population_sums, columns=['buffer_population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount of zero's\n",
    "(population_sums_df>0).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding population per buffer to buffer_gdf and then calculating density\n",
    "bufferintersect_gdf['buffer_area'] = bufferintersect_gdf['geometry'].area\n",
    "\n",
    "# adding both buffer population and buffer area columns to final gdf for other density calculations\n",
    "popgridmanchester['buffer_population'] = population_sums_df['buffer_population']\n",
    "popgridmanchester['buffer_area'] = bufferintersect_gdf['buffer_area']\n",
    "\n",
    "# calculating population density per buffer\n",
    "popgridmanchester['buffer_pop_density'] = popgridmanchester['buffer_population']/popgridmanchester['buffer_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popgridmanchester = popgridmanchester.drop(columns=['centroid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to gpkg\n",
    "popgridmanchester.to_file(\"popgridmanchester3.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting street networks per grid and retrieving network statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download whole street network of Greater Manchester from osmnx and project to CRS 27700\n",
    "G = ox.graph_from_place('Greater Manchester', network_type='all', simplify=True)\n",
    "Gproj = ox.project_graph(G, to_crs=\"27700\")\n",
    "\n",
    "# Save to graphml\n",
    "ox.io.save_graphml(Gproj, 'network.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open graphml\n",
    "Gproj = ox.load_graphml('network.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subgraphs for each grid instead of extracting the network per grid, seems to be incredibly fast!\n",
    "# # use the nodes from the whole street network of Greater Manchester, calculate their intersections \n",
    "# with buffer polygons and create the subgraph based on those nodes.\n",
    "def get_subgraph(polygon):\n",
    "    return Gproj.subgraph(Gproj_nodes[Gproj_nodes.intersects(polygon)].index)\n",
    "\n",
    "# get nodes from the Gproj \n",
    "Gproj_nodes = ox.graph_to_gdfs(Gproj, edges=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# applying the function with a list comprehension\n",
    "subgraph_list = [get_subgraph(polygon) for polygon in bufferintersect_gdf[0:999].geometry]\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subgraphs for each grid instead of extracting the network per grid, seems to be incredibly fast!\n",
    "# # use the nodes from the whole street network of Greater Manchester, calculate their intersections \n",
    "# with buffer polygons and create the subgraph based on those nodes.\n",
    "def get_subgraph(Gproj, Gproj_nodes, polygon):\n",
    "    return Gproj.subgraph(Gproj_nodes[Gproj_nodes.intersects(polygon)].index)\n",
    "\n",
    "# get nodes from the Gproj \n",
    "Gproj_nodes = ox.graph_to_gdfs(Gproj, edges=False)\n",
    "\n",
    "# applying function via multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    # initialize with less threads due to kernel crashing\n",
    "    subgraph_list = Parallel(n_jobs=2, prefer=\"threads\")(delayed(get_subgraph)(Gproj, Gproj_nodes, polygon) for polygon in bufferintersect_gdf[0:1].geometry)\n",
    "    # all cores\n",
    "    start_time = time.time()\n",
    "    subgraph_list = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(get_subgraph)(Gproj, Gproj_nodes, polygon) for polygon in bufferintersect_gdf.geometry)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the basic stats of every subgraph by turning the subgraph into a gdf, turning that into \n",
    "# one polygon and getting its area\n",
    "\n",
    "def get_subgraph_area(subgraph):\n",
    "    try:\n",
    "        return ox.graph_to_gdfs(subgraph, nodes=True, edges=False).unary_union.convex_hull.area\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "# applying the function via multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    # initialize with less threads due to kernel crashing\n",
    "    subgraph_area = Parallel(n_jobs=2, prefer=\"threads\")(delayed(get_subgraph_area)(subgraph) for subgraph in subgraph_list[0:1])\n",
    "    # all cores\n",
    "    start_time = time.time()\n",
    "    subgraph_area = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(get_subgraph_area)(subgraph) for subgraph in subgraph_list)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the statistics from each subgraph and area and accounting for the errors that might occur due\n",
    "# to a graph having no edges and giving a ValueError or due to the area being zero because the area\n",
    "# is not able to be calculated from a graph with no edges!\n",
    "\n",
    "def get_stats(subgraph, area):\n",
    "    try:\n",
    "        return ox.stats.basic_stats(subgraph, area)\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return 0\n",
    "\n",
    "# applying the function via multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    # initialize with less threads due to kernel crashing\n",
    "    subgraph_stats = Parallel(n_jobs=2, prefer=\"threads\")(delayed(get_stats)(subgraph, area) for subgraph, area in islice(zip(subgraph_list, subgraph_area), 0, 1))\n",
    "    # all cores\n",
    "    start_time = time.time()\n",
    "    subgraph_stats = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(get_stats)(subgraph, area) for subgraph, area in zip(subgraph_list, subgraph_area))\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving stats per subgraph, still took 23 hours..\n",
    "subgraph_stats = pd.DataFrame(subgraph_stats)\n",
    "subgraph_stats.to_pickle(\"subgraph_stats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file to gpkg\n",
    "#popgridmanchester.to_file('popgridmanchester2.gpkg', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing subgraph to corresponding network graph based on the grid centroid. Are similar!\n",
    "testgraph = ox.graph_from_point((popgridmanchester.loc[8447, 'grid_lat_4326'], \n",
    "                                 popgridmanchester.loc[8447, 'grid_lon_4326']), dist=500, network_type='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both subgraph and extracted graph from grid centroid\n",
    "ox.plot_graph(testgraph), ox.plot_graph(subgraph_list[8447])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "stats = ox.stats.basic_stats(subgraph_list[8447], subgraph_area[8447])\n",
    "stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-way-intersection density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance from grid centroids to nearest bus stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Download street network of Greater Manchester from osmnx and project to CRS 27700\n",
    "G = ox.graph_from_place('Greater Manchester', network_type='all', simplify=True)\n",
    "Gproj = ox.project_graph(G, to_crs=\"27700\")\n",
    "\n",
    "# Save to graphml\n",
    "ox.io.save_graphml(Gproj, 'network.graphml')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open graphml\n",
    "Gproj = ox.load_graphml('network.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd() #Location where files will be saved\n",
    "c = \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub df with lat/lon values to get their nearest nodes for truncation\n",
    "popgridsub = popgridmanchester.loc[200000:201000, ['geometry', 'grid_lon', 'grid_lat', 'grid_lon_4326', 'grid_lat_4326']]\n",
    "popgridsub = gpd.GeoDataFrame(popgridsub, geometry='geometry')\n",
    "popgridsub = popgridsub.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bus stops\n",
    "bus_stops = ox.geometries_from_place(\"Greater Manchester\", tags={'highway': 'bus_stop'})\n",
    "bus_stops = bus_stops.to_crs(\"EPSG:27700\")\n",
    "bus_stops = bus_stops[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate function to get shortest path\n",
    "def calculate_shortest_path(row):\n",
    "    try:\n",
    "        return nx.shortest_path_length(Gproj, valid_nodes_grid[row], valid_nodes_bus[row], weight='length')\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 10.778831005096436 seconds\n"
     ]
    }
   ],
   "source": [
    "# vectorized, multiprocessed and adjusted version of getting the shortest distance to busstops. MUCH faster.\n",
    "\n",
    "# function that is called in the multiprocessing joblib Parallel function\n",
    "def get_nearest_polygon(row):\n",
    "    return bus_stops.distance(row, align=True).sort_values().index[0]\n",
    "\n",
    "# timer start\n",
    "start_time = time.time()\n",
    "\n",
    "# get nearest bus stops based on euclid distance, now via multiprocessing\n",
    "# Apply multiprocessing using joblib to get the nearest bus stop to each grid polygon\n",
    "if __name__ == '__main__':\n",
    "    # initialization with less threads and one row due to kernel crashing otherwise\n",
    "    polygon_index = Parallel(n_jobs=2, prefer=\"threads\")(delayed(get_nearest_polygon)(row)\n",
    "                                                         for row in popgridsub['geometry'][0:1])\n",
    "    # all cores and rows\n",
    "    polygon_index = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(get_nearest_polygon)(row) \n",
    "                                                          for row in popgridsub['geometry'])\n",
    "\n",
    "# get the centroid of the nearest bus stop for each grid polygon\n",
    "nearest_bus = bus_stops.loc[polygon_index].geometry.centroid\n",
    "\n",
    "# list of grid centroid nearest nodes and their distances\n",
    "nearest_nodes, dist = ox.distance.nearest_nodes(Gproj, popgridsub['grid_lon'],\n",
    "                                                popgridsub['grid_lat'], return_dist=True)\n",
    "\n",
    "# using vectorization instead of iterations through for-loops to get the nearest grid nodes with\n",
    "# a threshold. MUCH faster.\n",
    "mask = np.vectorize(isinstance)(nearest_nodes, int)\n",
    "nearest_nodes_grid = np.where(mask, nearest_nodes, np.nan)\n",
    "dist_grid = np.where(mask, dist, np.nan)\n",
    "valid_nodes_grid = np.array([nearest_nodes_grid, dist_grid], np.float64)\n",
    "valid_nodes_grid = np.where(valid_nodes_grid[1]<1000, valid_nodes_grid[0], np.nan)\n",
    "\n",
    "# list of bus stop nearest nodes and their distances\n",
    "nearest_nodes_bus, dist_bus = ox.distance.nearest_nodes(Gproj, nearest_bus.geometry.x,\n",
    "                                                nearest_bus.geometry.y, return_dist=True)\n",
    "\n",
    "# using vectorization instead of iterations through for-loops to get the nearest bus stop nodes with\n",
    "# a threshold. MUCH faster.\n",
    "mask = np.vectorize(isinstance)(nearest_nodes_bus, int)\n",
    "nearest_nodes_b = np.where(mask, nearest_nodes_bus, np.nan)\n",
    "dist_bus = np.where(mask, dist_bus, np.nan)\n",
    "valid_nodes_bus = np.array([nearest_nodes_bus, dist_bus], np.float64)\n",
    "valid_nodes_bus = np.where(valid_nodes_bus[1]<1000, valid_nodes_bus[0], np.nan)\n",
    "\n",
    "# Get the shortest distance by calling the function one cell above that uses nx.shortest_path_length()\n",
    "popgridsub['dist_to_busstop'] = [calculate_shortest_path(x) for x in popgridsub.index]\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_file('popgriddistance.gpkg', driver='GPKG')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay with health outcome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the crs to the health data crs\n",
    "popgridmanchester = popgridmanchester.to_crs(\"epsg:27700\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create overlay with the health outcome data\n",
    "overlay = data_health.overlay(popgridmanchester, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save overlay\n",
    "overlay.to_file(\"overlay.gpkg\", driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4fefabefd88c249e4f3218e5e8556846ec1b0550b646e9eb707687c9edb20c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
